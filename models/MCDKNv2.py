import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Union, List, Tuple, Optional
import numpy as np
from sklearn.manifold import Isomap  # ç”¨äºè¿‘ä¼¼åŸç©ºé—´æµ‹åœ°çº¿è·ç¦»ï¼ˆğŸ”¶2-77èŠ‚ï¼‰

# 1. ä¿ç•™åŸFlowVectorFieldï¼Œä½†ä¿®æ­£è¾“å…¥ç»´åº¦å¯¹é½è®ºæ–‡å¾®åˆ†åŒèƒšå®šä¹‰ï¼ˆğŸ”¶2-49èŠ‚ï¼‰
class FlowVectorField(nn.Module):
    """PFMæ ¸å¿ƒç»„ä»¶ï¼šNeural ODEå‘é‡åœºï¼ˆé€‚é…è®ºæ–‡M=M_d'Ã—R^(d-d')ç»“æ„ï¼ŒğŸ”¶2-49ã€ğŸ”¶2-35èŠ‚ï¼‰"""
    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim + 1, hidden_dim),  # +1æ—¶é—´åµŒå…¥ï¼ˆè®ºæ–‡é»˜è®¤æ“ä½œï¼ŒğŸ”¶2-50èŠ‚ï¼‰
            nn.SiLU(),  # è®ºæ–‡ç”¨Swish/SiLUæ¿€æ´»ï¼ŒğŸ”¶2-50èŠ‚
            nn.Linear(hidden_dim, hidden_dim),
            nn.SiLU(),
            nn.Linear(hidden_dim, output_dim)
        )
        
    def forward(self, t: torch.Tensor, x: torch.Tensor) -> torch.Tensor:
        t_expanded = t.expand(x.shape[0], 1)
        tx = torch.cat([t_expanded, x], dim=1)
        return self.net(tx)

# 2. ä¿ç•™ode_solveï¼Œç¡®ä¿RK4æ±‚è§£å™¨å¯¹é½è®ºæ–‡ï¼ˆğŸ”¶2-52èŠ‚ç”¨Runge-Kuttaï¼‰
def ode_solve(func: nn.Module, x0: torch.Tensor, t_span: torch.Tensor, 
              method: str = "rk4", step_size: float = 0.01) -> torch.Tensor:
    device = x0.device
    solutions = [x0]
    x = x0.clone()
    for i in range(len(t_span) - 1):
        t0, t1 = t_span[i], t_span[i + 1]
        dt = t1 - t0
        if method == "rk4":
            k1 = func(t0, x)
            k2 = func(t0 + dt/2, x + dt/2 * k1)
            k3 = func(t0 + dt/2, x + dt/2 * k2)
            k4 = func(t1, x + dt * k3)
            x = x + dt/6 * (k1 + 2*k2 + 2*k3 + k4)
        else:
            x = x + dt * func(t0, x)
    return torch.stack(solutions, dim=0)

class PFM_DKN(nn.Module):
    """èåˆPFMæ€æƒ³çš„æ”¹è¿›DKNç½‘ç»œï¼ˆå…¨é‡å¯¹é½è®ºæ–‡ï¼ŒğŸ”¶2-22ã€ğŸ”¶2-38ã€ğŸ”¶2-45èŠ‚ï¼‰"""
    def __init__(self, x_dim: int, u_dim: int, hidden_dim: int, manifold_dim: int, 
                 latent_dim: int,  # æ½œæµå½¢ç»´åº¦=M_d'ç»´åº¦+å†—ä½™ç»´åº¦ï¼ˆğŸ”¶2-35èŠ‚M=M_d'Ã—R^(d-d')ï¼‰
                 state_low: Union[List[float], np.ndarray], 
                 state_high: Union[List[float], np.ndarray], 
                 action_low: Union[List[float]], 
                 action_high: Union[List[float]], 
                 dij,
                 device: torch.device):
        super().__init__()
        self.x_dim = x_dim
        self.u_dim = u_dim
        self.manifold_dim = manifold_dim  # M_d'ç»´åº¦ï¼ˆè®ºæ–‡é»˜è®¤d'=1ï¼ŒğŸ”¶2-74èŠ‚ï¼‰
        self.latent_dim = latent_dim      # æ½œæµå½¢æ€»ç»´åº¦ï¼ˆ=manifold_dim + å†—ä½™ç»´åº¦ï¼‰
        self.hidden_dim = hidden_dim
        self.device = device
        
        # çŠ¶æ€/åŠ¨ä½œèŒƒå›´ï¼ˆå¯¹é½è®ºæ–‡ç‰©ç†çº¦æŸï¼ŒğŸ”¶2-82èŠ‚ï¼‰
        self.state_low = torch.tensor(state_low, dtype=torch.float32, device=device)
        self.state_high = torch.tensor(state_high, dtype=torch.float32, device=device)
        self.action_low = torch.tensor(action_low, dtype=torch.float32, device=device)
        self.action_high = torch.tensor(action_high, dtype=torch.float32, device=device)
        
        # 3. ä¿®æ­£PFMæµå½¢æ˜ å°„ï¼šå¯¹é½è®ºæ–‡å¾®åˆ†åŒèƒšç»“æ„Ï†=[Ïˆâ»Â¹,I]âˆ˜Ï†âˆ˜T_Î¼ï¼ˆğŸ”¶2-48èŠ‚ï¼‰
        # 3.1 çŠ¶æ€â†’ä½ç»´æµå½¢M_d'ï¼ˆÏˆâ»Â¹å¯¹åº”éƒ¨åˆ†ï¼‰
        self.state_to_manifold = nn.Sequential(
            nn.Linear(x_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, manifold_dim)  # è¾“å‡ºM_d'ç»´åº¦
        )
        # 3.2 æµå½¢â†’æ½œæµå½¢Mï¼ˆM_d'Ã—R^(d-d')ï¼ŒIå¯¹åº”å†—ä½™ç»´åº¦æ’ç­‰æ˜ å°„ï¼‰
        self.manifold_to_latent = nn.Sequential(
            nn.Linear(manifold_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, latent_dim)  # æ½œæµå½¢æ€»ç»´åº¦ï¼ˆå«å†—ä½™ï¼‰
        )
        # 3.3 æ½œæµå½¢â†’æµå½¢ï¼ˆÏ†é€†æ˜ å°„ï¼ŒğŸ”¶2-48èŠ‚ï¼‰
        self.latent_to_manifold = nn.Sequential(
            nn.Linear(latent_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, manifold_dim)
        )
        # 3.4 PFMæ ¸å¿ƒï¼šNeural ODEå‚æ•°åŒ–å¾®åˆ†åŒèƒšÏ†ï¼ˆğŸ”¶2-49èŠ‚ï¼‰
        self.flow_encoder = FlowVectorField(latent_dim, hidden_dim, latent_dim)  # M_d'â†’M
        self.flow_decoder = FlowVectorField(latent_dim, hidden_dim, latent_dim)  # Mâ†’M_d'
        
        # 4. é«˜ç»´Koopmanç®—å­ï¼ˆå¯¹é½è®ºæ–‡ğŸ”¶2-40èŠ‚ï¼ŒA/Bä½œç”¨äºæ½œæµå½¢ï¼‰
        self.A = nn.Linear(latent_dim, latent_dim, bias=False)
        self.B = nn.Linear(u_dim, latent_dim, bias=False)
        
        # 5. æ§åˆ¶ç½‘ç»œï¼ˆä¿ç•™åŸç»“æ„ï¼Œå¯¹é½è®ºæ–‡ğŸ”¶2-21èŠ‚æ§åˆ¶åµŒå…¥ï¼‰
        self.control_net = nn.Sequential(
            nn.Linear(x_dim + u_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, u_dim)
        )
        self.inv_control_net = nn.Sequential(
            nn.Linear(x_dim + u_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, u_dim)
        )
        
        # 6. çŠ¶æ€æ¢å¤ï¼ˆå¯¹é½è®ºæ–‡ğŸ”¶2-35èŠ‚ï¼Œä»M_d'â†’åŸçŠ¶æ€ï¼‰
        self.manifold_to_state = nn.Sequential(
            nn.Linear(manifold_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, x_dim)
        )

        self.dij = dij
        self.dij_max = dij.max()
        self.dij_diff_max =  (dij - dij.T).abs().max()

        # 7. æ‹‰å›åº¦é‡å‚æ•°ï¼ˆå¯¹é½è®ºæ–‡ğŸ”¶2-33èŠ‚ï¼ŒåŸºäºæ½œæµå½¢åº¦é‡ï¼‰
        self.metric_scale = nn.Parameter(torch.ones(1, device=device))
        self.t_span = torch.linspace(0, 1, 5, device=device)  # è®ºæ–‡é»˜è®¤5ä¸ªæ—¶é—´æ­¥ï¼ŒğŸ”¶2-52èŠ‚
        
        # 8. è®ºæ–‡å®éªŒè¶…å‚ï¼ˆğŸ”¶2-246èŠ‚Table 5ï¼‰
        self.alpha1 = 1.0    # å…¨å±€ç­‰è·æŸå¤±æƒé‡
        self.alpha3 = 1.0    # å­æµå½¢æŸå¤±æƒé‡
        self.alpha4 = 0.001  # ç¨³å®šæ€§æ­£åˆ™åŒ–æƒé‡
        self.n_neighbors = 10 # Isomapè¿‘é‚»æ•°ï¼ˆğŸ”¶2-74èŠ‚ï¼‰

    def normalize_x(self, x: torch.Tensor) -> torch.Tensor:
        """çŠ¶æ€å½’ä¸€åŒ–ï¼ˆä¿ç•™åŸé€»è¾‘ï¼Œå¯¹é½è®ºæ–‡æ•°æ®é¢„å¤„ç†ï¼ŒğŸ”¶2-62èŠ‚ï¼‰"""
        x_clamped = torch.clamp(x, self.state_low, self.state_high)
        return (x_clamped - self.state_low) / (self.state_high - self.state_low + 1e-8)
    
    def normalize_u(self, u: torch.Tensor) -> torch.Tensor:
        """æ§åˆ¶å½’ä¸€åŒ–ï¼ˆåŒä¸Šï¼‰"""
        u_clamped = torch.clamp(u, self.action_low, self.action_high)
        return (u_clamped - self.action_low) / (self.action_high - self.action_low + 1e-8)
    
    # 9. ä¿®æ­£embed_to_latentï¼šå¯¹é½è®ºæ–‡å¾®åˆ†åŒèƒšæµç¨‹ï¼ˆğŸ”¶2-48ã€ğŸ”¶2-49èŠ‚ï¼‰
    def embed_to_latent(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """M_d'â†’Mï¼šä½ç»´æµå½¢â†’æ½œæµå½¢ï¼ˆå«Neural ODEæµï¼‰"""
        z_manifold = self.state_to_manifold(self.normalize_x(x))  # [B, d']
        redundant_zero = torch.zeros(
        size=(z_manifold.shape[0], self.latent_dim - self.manifold_dim),  # [B, d-d']
        dtype=torch.float32,
        device=self.device
        )
        # æ‹¼æ¥ä½ç»´å­æµå½¢ä¸å†—ä½™ç»´åº¦ï¼Œå¾—åˆ°é«˜ç»´æ½œæµå½¢åˆå§‹çŠ¶æ€ï¼ˆ[B, d]ï¼‰
        z_manifold = torch.cat([z_manifold, redundant_zero], dim=1)  # [B, d' + (d-d')] = [B, d]
        # Neural ODEç”Ÿæˆå¾®åˆ†åŒèƒšï¼ˆğŸ”¶2-49èŠ‚ï¼šÏ†é€šè¿‡Neural ODEå‚æ•°åŒ–ï¼‰
        z_latent = ode_solve(
            self.flow_encoder, 
            z_manifold, 
            self.t_span
        )[-1]  # [B, latent_dim]ï¼ˆM_d'Ã—R^(d-d')ï¼‰
        return z_latent, z_manifold
    
    # 10. ä¿®æ­£recover_from_latentï¼šå¯¹é½è®ºæ–‡é€†æ˜ å°„ï¼ˆğŸ”¶2-48èŠ‚Ï†â»Â¹ï¼‰
    def recover_from_latent(self, z_latent: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """Mâ†’M_d'â†’åŸçŠ¶æ€ï¼šæ½œæµå½¢â†’ä½ç»´æµå½¢â†’çŠ¶æ€"""
        # åå‘Neural ODEï¼ˆğŸ”¶2-51èŠ‚ï¼šé€†æ˜ å°„éœ€åå‘æ—¶é—´ç§¯åˆ†ï¼‰
        z_manifold = ode_solve(
            self.flow_decoder, 
            z_latent, 
            self.t_span.flip(0)
        )[-1]  # [B, d]
        z_manifold = z_manifold[:, :self.manifold_dim]
        z_manifold_ = z_manifold[:, self.manifold_dim:]  # æå–M_d'éƒ¨åˆ†
        x_recon = self.manifold_to_state(z_manifold)  # [B, x_dim]
        return x_recon, z_manifold_
    
    # 11. ä¿®æ­£pullback_metricï¼šå¯¹é½è®ºæ–‡æ‹‰å›åº¦é‡å®šä¹‰ï¼ˆğŸ”¶2-33èŠ‚å…¬å¼(21)ï¼‰
    def pullback_metric(self, z_manifold: torch.Tensor) -> torch.Tensor:
        """åŸºäºæ½œæµå½¢Mçš„æ‹‰å›åº¦é‡ï¼š(Î,Î¦)^Ï† = (Ï†_*[Î],Ï†_*[Î¦])^M"""
        B = z_manifold.shape[0]
        # æ½œæµå½¢Mçš„åº¦é‡ï¼ˆè®ºæ–‡é»˜è®¤M_d'ä¸ºæ¬§æ°ï¼Œå†—ä½™ç»´åº¦ä¸ºæ¬§æ°ï¼ŒğŸ”¶2-219èŠ‚ï¼‰
        metric_M = torch.eye(self.latent_dim, device=self.device).unsqueeze(0).repeat(B, 1, 1)  # [B, L, L]
        # è®¡ç®—pushforward Ï†_*ï¼ˆç®€åŒ–ï¼šç”¨Neural ODEé›…å¯æ¯”è¿‘ä¼¼ï¼ŒğŸ”¶2-215èŠ‚ï¼‰
        J = torch.autograd.functional.jacobian(
            lambda x: self.flow_encoder(t=torch.tensor(0.5, device=self.device), x=x),
            z_manifold
        ).squeeze(1)  # [B, L, d']
        # æ‹‰å›åº¦é‡ï¼šJ^T Â· metric_M Â· Jï¼ˆğŸ”¶2-33èŠ‚å…¬å¼ï¼‰
        pullback_metric = torch.matmul(torch.matmul(J.transpose(1,2), metric_M), J)  # [B, d', d']
        return pullback_metric * self.metric_scale
    
    # 12. é‡å†™isometry_lossï¼šèåˆè®ºæ–‡3å¤§ç­‰è·çº¦æŸï¼ˆğŸ”¶2-59èŠ‚å…¬å¼ï¼‰
    def isometry_loss(self, x: torch.Tensor, z_latent: torch.Tensor) -> torch.Tensor:
        """
        æ€»ç­‰è·æŸå¤± = å…¨å±€ç­‰è·æŸå¤± + å­æµå½¢æŸå¤± + ç¨³å®šæ€§æ­£åˆ™åŒ–
        æ–‡æ¡£ä¾æ®ï¼šğŸ”¶2-59ï¼ˆæŸå¤±å…¬å¼ï¼‰ã€ğŸ”¶2-68ï¼ˆÎµ_iso/Îµ_ldæŒ‡æ ‡ï¼‰ã€ğŸ”¶2-77ï¼ˆIsomapï¼‰
        """
        B = x.shape[0]
        z_manifold1 = self.latent_to_manifold(z_latent[:, :self.latent_dim])  # [B, d']
        z_manifold2 = self.latent_to_manifold(z_latent[:, self.latent_dim:])  # [B, d']
        # 12.1 å…¨å±€ç­‰è·æŸå¤±ï¼ˆğŸ”¶2-59èŠ‚ç¬¬ä¸€é¡¹ï¼‰ï¼šd_D(xi,xj) â‰ˆ d_M(Ï†(xi),Ï†(xj))
        # åŸç©ºé—´çœŸå®è·ç¦»d_ijï¼šIsomapè¿‘ä¼¼ï¼ˆğŸ”¶2-77èŠ‚æ ‡å‡†åšæ³•ï¼‰
        x_norm1 = self.normalize_x(x[:, :self.x_dim])
        x_norm2 = self.normalize_x(x[:, self.x_dim:])
        dij
        
        # æ½œç©ºé—´è·ç¦»d_Mï¼šå¯¹é½è®ºæ–‡å…¬å¼(2)ï¼ˆğŸ”¶2-33èŠ‚ï¼‰
        d_M = torch.cdist(z_latent, z_latent)  # [B, B]
        global_loss = self.alpha1 * torch.mean(torch.square(d_ij - d_M))  # å¹³æ–¹æŸå¤±ï¼ˆè®ºæ–‡å®šä¹‰ï¼‰
        
        # 12.2 å­æµå½¢æŸå¤±ï¼ˆğŸ”¶2-59èŠ‚ç¬¬ä¸‰é¡¹ï¼‰ï¼šå¼ºåˆ¶æ½œæµå½¢æ˜ å°„åˆ°M_d'ï¼ˆå†—ä½™ç»´åº¦â†’0ï¼‰
        mask = torch.zeros_like(z_latent, device=self.device)
        mask[:, :self.manifold_dim] = 1.0  # ä¿ç•™M_d'ç»´åº¦ï¼Œæ©ç å†—ä½™ç»´åº¦
        redundant_dim = z_latent * (1 - mask)  # [B, L-d']
        submanifold_loss = self.alpha3 * torch.mean(torch.norm(redundant_dim, p=1, dim=1))
        
        # 12.3 ç¨³å®šæ€§æ­£åˆ™åŒ–ï¼ˆğŸ”¶2-59èŠ‚ç¬¬å››é¡¹ï¼‰ï¼šå±€éƒ¨ç­‰è·çº¦æŸ
        stability_loss = torch.tensor(0.0, device=self.device)
        if B > 1:
            # å±€éƒ¨è¿‘é‚»è·ç¦»ä¸€è‡´æ€§ï¼ˆç®€åŒ–é›…å¯æ¯”æ­£åˆ™åŒ–ï¼ŒğŸ”¶2-60èŠ‚æ€æƒ³ï¼‰
            z_dist = torch.cdist(z_manifold1, z_manifold2)
            # z_dist = z_dist.fill_diagonal_(float('inf'))
            _, nn_idx = torch.min(z_dist, dim=1)  # æ¯ä¸ªæ ·æœ¬çš„1è¿‘é‚»
            x_nn_dist = d_ij[torch.arange(B), nn_idx]  # åŸç©ºé—´è¿‘é‚»è·ç¦»
            z_nn_dist = z_dist[torch.arange(B), nn_idx]  # æ½œç©ºé—´è¿‘é‚»è·ç¦»
            stability_loss = self.alpha4 * torch.mean(torch.abs(x_nn_dist - z_nn_dist))
        
        # æ€»ç­‰è·æŸå¤±
        total_iso_loss = global_loss + submanifold_loss + stability_loss
        # è¾“å‡ºè®ºæ–‡æŒ‡æ ‡ï¼ˆğŸ”¶2-68èŠ‚ï¼‰
        with torch.no_grad():
            eps_iso = torch.mean(torch.abs(d_ij - d_M)).item()
            eps_ld = torch.mean(torch.norm(redundant_dim, p=1, dim=1)).item()
            print(f"Îµ_iso: {eps_iso:.6f} | Îµ_ld: {eps_ld:.6f}")
        return total_iso_loss
    
    # 13. ä¿®æ­£flow_matching_lossï¼šå¯¹é½è®ºæ–‡PFMç›®æ ‡å‡½æ•°ï¼ˆğŸ”¶2-40èŠ‚å…¬å¼(6)ï¼‰
    def flow_matching_loss(self, x0: torch.Tensor, x1: torch.Tensor) -> torch.Tensor:
        """PFMæµåŒ¹é…æŸå¤±ï¼šåŒ¹é…æ½œæµå½¢æµ‹åœ°çº¿çš„æ—¶é—´å¯¼æ•°"""
        z0, _ = self.embed_to_latent(x0)  # [B, L]
        z1, _ = self.embed_to_latent(x1)  # [B, L]
        
        # è®ºæ–‡ğŸ”¶2-40èŠ‚ï¼šæ—¶é—´è°ƒåº¦å™¨Îº(t)ï¼ˆå•è°ƒé€’å‡ï¼ŒÎº(0)=1, Îº(1)=0ï¼‰
        t = torch.rand(z0.shape[0], 1, device=self.device)  # [B, 1]
        kappa_t = 1 - t  # ç®€åŒ–è°ƒåº¦å™¨ï¼ˆè®ºæ–‡å¸¸ç”¨å½¢å¼ï¼‰
        
        # æ½œæµå½¢æµ‹åœ°çº¿ï¼ˆè®ºæ–‡å…¬å¼(3)ï¼šÎ³^Ï† = Ï†â»Â¹(Î³^M)ï¼Œæ­¤å¤„Mä¸ºæ¬§æ°ï¼Œæµ‹åœ°çº¿ä¸ºçº¿æ€§æ’å€¼ï¼‰
        z_t = torch.lerp(z0, z1, kappa_t)  # [B, L]
        
        # ç›®æ ‡å‘é‡åœºï¼šæµ‹åœ°çº¿æ—¶é—´å¯¼æ•°ï¼ˆğŸ”¶2-40èŠ‚ï¼šáº‹_t = Îºâ€™(t)Â·log_x1(x0)ï¼Œç®€åŒ–ä¸ºz1-z0ï¼‰
        kappa_prime_t = -1.0  # Îº(t)=1-tçš„å¯¼æ•°
        u_t = kappa_prime_t * (z0 - z1)  # [B, L]
        
        # æ¨¡å‹é¢„æµ‹å‘é‡åœºï¼ˆè®ºæ–‡ç”¨ç¥ç»ç½‘ç»œå‚æ•°åŒ–v_tï¼Œéç›´æ¥ç”¨A(z_t)ï¼‰
        # ä¿®æ­£ï¼šæ–°å¢å‘é‡åœºç½‘ç»œï¼ˆåŸæ¨¡å‹ç”¨A(z_t)é”™è¯¯ï¼ŒğŸ”¶2-40èŠ‚è¦æ±‚ç‹¬ç«‹v_tï¼‰
        if not hasattr(self, 'flow_vector_net'):
            self.flow_vector_net = nn.Sequential(
                nn.Linear(self.latent_dim + 1, self.hidden_dim),
                nn.SiLU(),
                nn.Linear(self.hidden_dim, self.latent_dim)
            ).to(self.device)
        # æ—¶é—´åµŒå…¥ï¼ˆğŸ”¶2-50èŠ‚ï¼šå‘é‡åœºéœ€è¾“å…¥tï¼‰
        t_expanded = t.expand_as(z_t[:, :1])
        vt_input = torch.cat([z_t, t_expanded], dim=1)  # [B, L+1]
        v_t = self.flow_vector_net(vt_input)  # [B, L]
        
        # æµåŒ¹é…æŸå¤±ï¼ˆè®ºæ–‡å…¬å¼(6)ï¼šL2æŸå¤±ï¼‰
        return torch.mean(torch.norm(v_t - u_t, dim=1))
    
    # 14. ä¿ç•™æ§åˆ¶ç½‘ç»œï¼ˆæ— ä¿®æ”¹ï¼Œå¯¹é½è®ºæ–‡æ§åˆ¶åµŒå…¥é€»è¾‘ğŸ”¶2-21èŠ‚ï¼‰
    def forward_control(self, x: torch.Tensor, u: torch.Tensor) -> torch.Tensor:
        if len(x.shape) < 2:
            x = x.unsqueeze(0)
        if len(u.shape) < 2:
            u = u.unsqueeze(0)
        x_u = torch.cat([self.normalize_x(x), self.normalize_u(u)], dim=1)
        return self.control_net(x_u)
    
    def forward_inv_control(self, x: torch.Tensor, hat_u: torch.Tensor) -> torch.Tensor:
        if len(x.shape) < 2:
            x = x.unsqueeze(0)
        if len(hat_u.shape) < 2:
            hat_u = hat_u.unsqueeze(0)
        x_hat_u = torch.cat([self.normalize_x(x), self.normalize_u(hat_u)], dim=1)
        return self.inv_control_net(x_hat_u)
    
    # 15. ä¿ç•™Koopmanæ¼”åŒ–ï¼ˆæ— ä¿®æ”¹ï¼Œå¯¹é½è®ºæ–‡ğŸ”¶2-40èŠ‚ï¼‰
    def forward_koopman(self, z_latent: torch.Tensor, g_phi: torch.Tensor) -> torch.Tensor:
        return self.A(z_latent) + self.B(g_phi)
    
    # 16. ä¿ç•™å¤šæ­¥é¢„æµ‹ï¼ˆæ— ä¿®æ”¹ï¼Œå¯¹é½è®ºæ–‡ğŸ”¶2-85èŠ‚ç”Ÿæˆé€»è¾‘ï¼‰
    def predict_k_steps(self, x0: torch.Tensor, u_seq: torch.Tensor, k: int) -> torch.Tensor:
        batch_size = x0.shape[0]
        x_seq = [x0]
        z_latent_prev, _ = self.embed_to_latent(x0)
        z_manifold_sum = 0.0  # ç”¨äºç›‘æ§å†—ä½™ç»´åº¦
        for t in range(k):
            u_t = u_seq[t].view(batch_size, self.u_dim)
            g_phi_t = self.forward_control(x_seq[-1], u_t)
            z_latent_next = self.forward_koopman(z_latent_prev, g_phi_t)
            x_next, tool = self.recover_from_latent(z_latent_next)
            x_seq.append(x_next)
            z_manifold_sum += torch.mean(torch.abs(tool)).item()
            z_latent_prev = z_latent_next
        return torch.stack(x_seq, dim=0), z_manifold_sum

# 17. ä¿®æ­£compute_pfm_total_lossï¼šå¯¹é½è®ºæ–‡æŸå¤±æƒé‡ï¼ˆğŸ”¶2-246èŠ‚ï¼‰
def compute_pfm_total_loss(
    model: PFM_DKN,
    x_prev: torch.Tensor,
    x_next: torch.Tensor,
    u_prev: torch.Tensor,
    u0: torch.Tensor,
    lambda_L1: float = 1.0,    # è®ºæ–‡é»˜è®¤1.0
    lambda_L2: float = 1.0,    # è®ºæ–‡é»˜è®¤1.0
    lambda_isometry: float = 1.0,  # å¯¹é½Î±1=1.0
    lambda_flow: float = 0.5,  # æµåŒ¹é…æŸå¤±æƒé‡
    **kwargs
) -> Tuple[torch.Tensor, dict]:
    """èåˆè®ºæ–‡PFMæ€»æŸå¤±ï¼ˆğŸ”¶2-59ã€ğŸ”¶2-40èŠ‚ï¼‰"""
    # æ½œç©ºé—´è¡¨ç¤ºï¼ˆä¿®æ­£åŸæ¨¡å‹xz_prev/xz_nexté”™è¯¯ï¼Œåº”ä¸ºz_prev/z_nextï¼‰
    z_prev, _ = model.embed_to_latent(x_prev)
    z_next, _ = model.embed_to_latent(x_next)
    
    # 17.1 L1æŸå¤±ï¼ˆé«˜ç»´Koopmané¢„æµ‹è¯¯å·®ï¼Œä¿ç•™åŸé€»è¾‘ï¼‰
    v_prev = u_prev - u0
    z_next_pred = torch.matmul(z_prev, model.A.weight.T) + torch.matmul(v_prev, model.B.weight.T)
    L1 = torch.mean(torch.norm(z_next - z_next_pred, p='fro', dim=1))
    
    # 17.2 L2æŸå¤±ï¼ˆèƒ½æ§æ€§+æ­£åˆ™åŒ–ï¼Œä¿ç•™åŸé€»è¾‘ï¼ŒğŸ”¶2-59èŠ‚ç¬¬äºŒé¡¹ï¼‰
    from rdkrc.utils.matrix_utils import compute_controllability_matrix
    controllability_mat = compute_controllability_matrix(model.A.weight, model.B.weight)
    _, singular_vals, _ = torch.svd(controllability_mat)
    rank = (singular_vals > 1e-5).sum().item()
    N = model.A.weight.shape[0]
    rank_penalty = (N - rank) / N
    A_l1 = torch.norm(model.A.weight, p=1)
    B_l1 = torch.norm(model.B.weight, p=1)
    L2 = rank_penalty + A_l1 + B_l1
    
    # 17.3 ç­‰è·æŸå¤±ï¼ˆè°ƒç”¨ä¿®æ­£åçš„å‡½æ•°ï¼Œèåˆ3å¤§çº¦æŸï¼‰
    isometry_loss = model.isometry_loss(torch.cat([x_prev, x_next], dim=1), 
                                       torch.cat([z_prev, z_next], dim=1))
    
    # 17.4 æµåŒ¹é…æŸå¤±ï¼ˆè°ƒç”¨ä¿®æ­£åçš„å‡½æ•°ï¼Œå¯¹é½è®ºæ–‡å…¬å¼ï¼‰
    flow_loss = model.flow_matching_loss(x_prev, x_next)
    
    # 17.5 é‡æ„æŸå¤±ï¼ˆä¿ç•™åŸé€»è¾‘ï¼Œå¯¹é½è®ºæ–‡ğŸ”¶2-82èŠ‚çŠ¶æ€æ¢å¤ï¼‰
    x_prev_recon, _ = model.recover_from_latent(z_prev)
    x_next_recon, _ = model.recover_from_latent(z_next)
    recon_loss = 0.5 * (torch.mean(torch.norm(x_prev - x_prev_recon, dim=1)) +
                       torch.mean(torch.norm(x_next - x_next_recon, dim=1)))
    
    # 17.6 æ€»æŸå¤±ï¼ˆæƒé‡å¯¹é½è®ºæ–‡å®éªŒé…ç½®ï¼‰
    total_loss = (lambda_L1 * L1 +
                 lambda_L2 * L2 +
                 lambda_isometry * isometry_loss +
                 lambda_flow * flow_loss +
                 0.5 * recon_loss)
    
    # æŸå¤±ç›‘æ§ï¼ˆä¿ç•™åŸé€»è¾‘ï¼‰
    loss_components = {
        'total': total_loss.item(),
        'L1': L1.item(),
        'L2': L2.item(),
        'isometry': isometry_loss.item(),
        'flow': flow_loss.item(),
        'recon': recon_loss.item()
    }
    return total_loss, loss_components