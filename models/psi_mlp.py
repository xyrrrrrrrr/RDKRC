import torch
import torch.nn as nn
import numpy as np
from typing import List, Optional, Union, Tuple


class PsiMLP(nn.Module):
    """
    åŸºäºMLPçš„KoopmanåŸºå‡½æ•°å­¦ä¹ ç½‘ç»œï¼ˆå«uâ‚€ä¼°è®¡ï¼‰ï¼Œå¯¹åº”æ–‡æ¡£ï¼š
    - Section II "Koopman Learning using Deep Neural Network"ï¼ˆåŸºå‡½æ•°DNNå®šä¹‰ï¼‰
    - Equation 4ï¼ˆz = Î¨(x) - Î¨(x*)ï¼‰
    - æ–‡æ¡£æåŠçš„â€œone additional auxiliary network for this constant uâ‚€â€ï¼ˆuâ‚€ä¼°è®¡ï¼‰
    
    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. å­¦ä¹ KoopmanåŸºå‡½æ•°Î¨(x)ï¼Œå°†åŸçŠ¶æ€xæ˜ å°„è‡³é«˜ç»´ç©ºé—´
    2. è®¡ç®—é«˜ç»´çº¿æ€§ç©ºé—´çŠ¶æ€z = Î¨(x) - Î¨(x*)ï¼ˆx*ä¸ºç›®æ ‡çŠ¶æ€ï¼‰
    3. å­¦ä¹ æ§åˆ¶è¾“å…¥å›ºå®šç‚¹uâ‚€ï¼ˆé€šè¿‡å¯å­¦ä¹ å‚æ•°å®ç°ï¼Œç¬¦åˆæ–‡æ¡£â€œå¸¸æ•°uâ‚€â€è¦æ±‚ï¼‰

    Args:
        input_dim (int): åŸå§‹çŠ¶æ€xçš„ç»´åº¦ï¼ˆå¦‚å€’ç«‹æ‘†n=3ï¼Œæœˆçƒç€é™†å™¨n=6ï¼‰
        output_dim (int): é«˜ç»´åŸºå‡½æ•°Î¨(x)çš„ç»´åº¦Nï¼ˆéœ€æ»¡è¶³N â‰« input_dimï¼Œæ–‡æ¡£é»˜è®¤é«˜ç»´ï¼‰
        control_dim (int): æ§åˆ¶è¾“å…¥uçš„ç»´åº¦mï¼ˆå¦‚æœˆçƒç€é™†å™¨m=2ï¼Œå€’ç«‹æ‘†m=1ï¼‰
        low (Union[List[float], np.ndarray]): åŸå§‹çŠ¶æ€xçš„å„ç»´åº¦ä¸‹ç•Œï¼ˆç”¨äºå½’ä¸€åŒ–ï¼Œå¤–éƒ¨ä¼ å…¥è§£è€¦ç¯å¢ƒï¼‰
        high (Union[List[float], np.ndarray]): åŸå§‹çŠ¶æ€xçš„å„ç»´åº¦ä¸Šç•Œï¼ˆç”¨äºå½’ä¸€åŒ–ï¼Œå¤–éƒ¨ä¼ å…¥è§£è€¦ç¯å¢ƒï¼‰
        hidden_dims (List[int], optional): åŸºå‡½æ•°MLPçš„éšè—å±‚ç»´åº¦ï¼ˆæ–‡æ¡£ç”¨4å±‚ï¼Œé»˜è®¤[256,256,256,256]ï¼‰
        activation (nn.Module, optional): éšè—å±‚æ¿€æ´»å‡½æ•°ï¼ˆæ–‡æ¡£ç”¨tanhï¼Œé»˜è®¤nn.Tanh()ï¼‰
        device (Optional[torch.device]): è®¡ç®—è®¾å¤‡ï¼ˆé»˜è®¤è‡ªåŠ¨æ£€æµ‹GPU/CPUï¼‰
    """
    def __init__(
        self,
        input_dim: int,
        output_dim: int,
        control_dim: int,
        low: Union[List[float], np.ndarray],
        high: Union[List[float], np.ndarray],
        hidden_dims: List[int] = None,
        activation: nn.Module = None,
        device: Optional[torch.device] = None
    ):
        super().__init__()
        # è®¾å¤‡åˆå§‹åŒ–ï¼ˆå¯¹é½æ–‡æ¡£GPUè®­ç»ƒè¦æ±‚ï¼Œå¦‚NVIDIA V100/DGX-2ï¼‰
        self.device = device or torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # æ ¸å¿ƒå‚æ•°ï¼ˆåŒ¹é…æ–‡æ¡£å®šä¹‰ï¼‰
        self.input_dim = input_dim  # åŸçŠ¶æ€ç»´åº¦n
        self.output_dim = output_dim  # é«˜ç»´ç©ºé—´ç»´åº¦N
        self.control_dim = control_dim  # æ§åˆ¶è¾“å…¥ç»´åº¦m
        
        # çŠ¶æ€å½’ä¸€åŒ–å‚æ•°ï¼ˆå¤–éƒ¨ä¼ å…¥ï¼Œè§£è€¦gymç¯å¢ƒï¼Œé¿å…æ¨¡å‹å†…ç¡¬ç¼–ç ï¼‰
        self.low = torch.tensor(low, dtype=torch.float32, device=self.device)
        self.high = torch.tensor(high, dtype=torch.float32, device=self.device)
        # æ£€æŸ¥å½’ä¸€åŒ–å‚æ•°ç»´åº¦ä¸è¾“å…¥ç»´åº¦ä¸€è‡´
        assert self.low.shape[0] == input_dim and self.high.shape[0] == input_dim, \
            f"low/highç»´åº¦éœ€ä¸input_dim({input_dim})ä¸€è‡´ï¼Œå½“å‰lowç»´åº¦{self.low.shape[0]}ï¼Œhighç»´åº¦{self.high.shape[0]}"
        
        # åŸºå‡½æ•°MLPç»“æ„ï¼ˆæ–‡æ¡£Section IIï¼š4å±‚éšè—å±‚+tanhæ¿€æ´»ï¼‰
        self.hidden_dims = hidden_dims or [256, 256, 256, 256]  # æ–‡æ¡£é»˜è®¤4å±‚éšè—å±‚
        self.activation = activation or nn.Tanh()  # æ–‡æ¡£æŒ‡å®štanhæ¿€æ´»
        self.psi_mlp = self._build_psi_mlp()
        
        # æ§åˆ¶è¾“å…¥å›ºå®šç‚¹uâ‚€ï¼ˆæ–‡æ¡£è¦æ±‚ï¼šé¢å¤–è¾…åŠ©ç½‘ç»œå­¦ä¹ å¸¸æ•°uâ‚€ï¼Œæ­¤å¤„ç”¨å¯å­¦ä¹ å‚æ•°ç®€åŒ–å®ç°ï¼Œç¬¦åˆâ€œå¸¸æ•°â€å±æ€§ï¼‰
        self.u0 = nn.Parameter(
            torch.zeros(control_dim, dtype=torch.float32, device=self.device),
            requires_grad=True  # å…è®¸åå‘ä¼ æ’­æ›´æ–°ï¼Œå®ç°æ•°æ®é©±åŠ¨å­¦ä¹ 
        )

    def _build_psi_mlp(self) -> nn.Sequential:
        """æ„å»ºåŸºå‡½æ•°Î¨(x)çš„MLPç½‘ç»œï¼ˆæ–‡æ¡£Section IIæŒ‡å®šç»“æ„ï¼‰"""
        layers = []
        in_dim = self.input_dim
        
        # éšè—å±‚ï¼ˆæŒ‰æ–‡æ¡£4å±‚è®¾è®¡ï¼‰
        for hidden_dim in self.hidden_dims:
            layers.append(nn.Linear(in_dim, hidden_dim, device=self.device))
            layers.append(self.activation)  # æ¯å±‚åæ¥tanhæ¿€æ´»
            in_dim = hidden_dim
        
        # è¾“å‡ºå±‚ï¼ˆæ— æ¿€æ´»ï¼Œç›´æ¥è¾“å‡ºé«˜ç»´åŸºå‡½æ•°å€¼Î¨(x)ï¼‰
        layers.append(nn.Linear(in_dim, self.output_dim, device=self.device))
        
        return nn.Sequential(*layers)

    def normalize_x(self, x: torch.Tensor) -> torch.Tensor:
        """
        çŠ¶æ€å½’ä¸€åŒ–ï¼ˆæ–‡æ¡£éšå«æ•°æ®é¢„å¤„ç†è¦æ±‚ï¼Œæå‡è®­ç»ƒç¨³å®šæ€§ï¼‰ï¼š
        å°†xæ˜ å°„åˆ°[0,1]åŒºé—´ï¼Œå…ˆè£å‰ªåˆ°[low, high]é¿å…å¼‚å¸¸å€¼å½±å“
        
        Args:
            x (torch.Tensor): åŸå§‹çŠ¶æ€æ‰¹é‡ï¼Œå½¢çŠ¶[batch_size, input_dim]
        
        Returns:
            torch.Tensor: å½’ä¸€åŒ–åçŠ¶æ€ï¼Œå½¢çŠ¶[batch_size, input_dim]
        """
        # ç¡®ä¿è¾“å…¥åœ¨è®¾å¤‡ä¸Šä¸”ç±»å‹åŒ¹é…
        x = x.to(self.device, dtype=torch.float32)
        # è£å‰ªå¼‚å¸¸å€¼åˆ°[low, high]
        x_clamped = torch.clamp(x, self.low, self.high)
        # å½’ä¸€åŒ–å…¬å¼ï¼š(x - low) / (high - low)
        x_norm = (x_clamped - self.low) / (self.high - self.low + 1e-8)  # åŠ 1e-8é¿å…åˆ†æ¯ä¸º0
        return x_norm

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­ï¼šè¾“å…¥åŸå§‹çŠ¶æ€xï¼Œè¾“å‡ºé«˜ç»´åŸºå‡½æ•°Î¨(x)ï¼ˆæ–‡æ¡£Section IIæ ¸å¿ƒæ˜ å°„ï¼‰
        
        Args:
            x (torch.Tensor): åŸå§‹çŠ¶æ€æ‰¹é‡ï¼Œå½¢çŠ¶[batch_size, input_dim]
        
        Returns:
            torch.Tensor: é«˜ç»´åŸºå‡½æ•°æ‰¹é‡ï¼Œå½¢çŠ¶[batch_size, output_dim]
        """
        # å…ˆå½’ä¸€åŒ–å†è¾“å…¥MLPï¼ˆç¬¦åˆæ•°æ®é©±åŠ¨è®­ç»ƒé€»è¾‘ï¼‰
        x_norm = self.normalize_x(x)
        return self.psi_mlp(x_norm)
        # return self.psi_mlp(x)

    def compute_z(self, x: torch.Tensor, x_star: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—é«˜ç»´çº¿æ€§ç©ºé—´çŠ¶æ€zï¼ˆæ–‡æ¡£Equation 4ï¼šz = Î¨(x) - Î¨(x*)ï¼‰
        x*ä¸ºç›®æ ‡çŠ¶æ€ï¼ˆå¦‚å€’ç«‹æ‘†Î¸=0,Î¸_dot=0ï¼›æœˆçƒç€é™†å™¨landing zoneï¼‰
        
        Args:
            x (torch.Tensor): åŸå§‹çŠ¶æ€æ‰¹é‡ï¼Œå½¢çŠ¶[batch_size, input_dim]
            x_star (torch.Tensor): ç›®æ ‡çŠ¶æ€ï¼ˆå•æ ·æœ¬ï¼‰ï¼Œå½¢çŠ¶[input_dim]æˆ–[1, input_dim]
        
        Returns:
            torch.Tensor: é«˜ç»´çŠ¶æ€zï¼Œå½¢çŠ¶[batch_size, output_dim]
        """
        # å¤„ç†x_starç»´åº¦ï¼šæ‰©å±•åˆ°æ‰¹é‡å¤§å°ï¼Œç¡®ä¿ä¸xç»´åº¦åŒ¹é…
        x_star = x_star.to(self.device, dtype=torch.float32)
        if x_star.dim() == 1:
            x_star = x_star.unsqueeze(0)  # [input_dim] â†’ [1, input_dim]
        x_star_batch = x_star.expand(x.shape[0], -1)  # [batch_size, input_dim]
        
        # è®¡ç®—z = Î¨(x) - Î¨(x*)
        psi_x = self.forward(x)
        psi_x_star = self.forward(x_star_batch)
        return psi_x - psi_x_star

    def forward_u0(self, x: torch.Tensor) -> torch.Tensor:
        """
        è¾“å‡ºæ§åˆ¶è¾“å…¥å›ºå®šç‚¹uâ‚€ï¼ˆæ–‡æ¡£è¦æ±‚ï¼šæ•°æ®é©±åŠ¨å­¦ä¹ çš„å¸¸æ•°uâ‚€ï¼‰
        æ‰©å±•uâ‚€åˆ°æ‰¹é‡ç»´åº¦ï¼Œç¡®ä¿ä¸çŠ¶æ€æ‰¹é‡å¤§å°åŒ¹é…ï¼ˆä¾¿äºåç»­æŸå¤±è®¡ç®—ï¼‰
        
        Args:
            x (torch.Tensor): åŸå§‹çŠ¶æ€æ‰¹é‡ï¼ˆä»…ç”¨äºè·å–æ‰¹é‡å¤§å°ï¼Œæ— å®é™…è¾“å…¥ä¾èµ–ï¼‰ï¼Œå½¢çŠ¶[batch_size, input_dim]
        
        Returns:
            torch.Tensor: uâ‚€æ‰¹é‡ï¼Œå½¢çŠ¶[batch_size, control_dim]
        """
        batch_size = x.shape[0]
        # æ‰©å±•å¸¸æ•°uâ‚€åˆ°æ‰¹é‡ç»´åº¦ï¼š[control_dim] â†’ [1, control_dim] â†’ [batch_size, control_dim]
        return self.u0.unsqueeze(0).expand(batch_size, -1)
    

class PsiMLP_v2(nn.Module):
    """
    æ”¹è¿›ç‰ˆKoopmanåŸºå‡½æ•°å­¦ä¹ ç½‘ç»œï¼ˆå¢å¼ºè¡¨è¾¾èƒ½åŠ›ï¼‰
    æ”¹è¿›ç‚¹ï¼š
    1. åŠ å…¥çŠ¶æ€æ³¨æ„åŠ›æœºåˆ¶ï¼Œè‡ªåŠ¨èšç„¦å…³é”®çŠ¶æ€ç»´åº¦
    2. é‡‡ç”¨æ··åˆæ¿€æ´»å‡½æ•°ï¼ˆtanh + Swishï¼‰ï¼Œç¼“è§£æ¢¯åº¦é¥±å’Œ
    3. å¤šå°ºåº¦ç‰¹å¾èåˆï¼Œå¢å¼ºå¯¹ä¸åŒé¢‘ç‡åŠ¨æ€çš„æ•æ‰èƒ½åŠ›
    """
    def __init__(
        self,
        input_dim: int,
        output_dim: int,
        control_dim: int,
        low: Union[List[float], np.ndarray],
        high: Union[List[float], np.ndarray],
        hidden_dims: List[int] = None,
        activation: nn.Module = None,
        device: Optional[torch.device] = None
    ):
        super().__init__()
        self.device = device or torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # æ ¸å¿ƒå‚æ•°ä¿æŒä¸å˜
        self.input_dim = input_dim  # åŸçŠ¶æ€ç»´åº¦nï¼ˆå¦‚æœˆçƒç€é™†å™¨n=6ï¼‰
        self.output_dim = output_dim  # é«˜ç»´ç©ºé—´ç»´åº¦N
        self.control_dim = control_dim  # æ§åˆ¶è¾“å…¥ç»´åº¦m
        
        # çŠ¶æ€å½’ä¸€åŒ–å‚æ•°ï¼ˆè§£è€¦ç¯å¢ƒä¾èµ–ï¼‰
        self.low = torch.tensor(low, dtype=torch.float32, device=self.device)
        self.high = torch.tensor(high, dtype=torch.float32, device=self.device)
        assert self.low.shape[0] == input_dim and self.high.shape[0] == input_dim, \
            f"low/highç»´åº¦éœ€ä¸input_dim({input_dim})ä¸€è‡´"
        
        # æ”¹è¿›1ï¼šçŠ¶æ€æ³¨æ„åŠ›æœºåˆ¶ï¼ˆèšç„¦å…³é”®çŠ¶æ€ç»´åº¦ï¼‰
        self.attention = nn.Sequential(
            nn.Linear(input_dim, input_dim, device=self.device),  # å­¦ä¹ çŠ¶æ€ç»´åº¦æƒé‡
            nn.Sigmoid()  # è¾“å‡ºæ³¨æ„åŠ›æƒé‡âˆˆ[0,1]
        )
        
        # æ”¹è¿›2ï¼šå¤šå°ºåº¦åˆ†æ”¯ç»“æ„ï¼ˆæ‹†åˆ†ä½ç»´/é«˜ç»´ç‰¹å¾ï¼‰
        self.hidden_dims = hidden_dims or [256, 256, 256, 256]
        # ä½ç»´åˆ†æ”¯ï¼ˆæ•æ‰æ…¢å˜ç‰¹å¾ï¼Œå¦‚ä½ç½®ã€é«˜åº¦ï¼‰
        self.branch_low = self._build_branch(
            input_dim=input_dim,
            hidden_dims=self.hidden_dims,
            output_dim=output_dim // 4  # åˆ†é…1/4ç»´åº¦ï¼ˆå¦‚256â†’64ï¼‰
        )
        # é«˜ç»´åˆ†æ”¯ï¼ˆæ•æ‰å¿«å˜ç‰¹å¾ï¼Œå¦‚è§’é€Ÿåº¦ã€é€Ÿåº¦ï¼‰
        self.branch_high = self._build_branch(
            input_dim=input_dim,
            hidden_dims=self.hidden_dims,
            output_dim=output_dim - (output_dim // 4)  # å‰©ä½™3/4ç»´åº¦ï¼ˆå¦‚256â†’192ï¼‰
        )
        
        # æ§åˆ¶è¾“å…¥å›ºå®šç‚¹uâ‚€ï¼ˆä¿æŒåŸè®¾è®¡ï¼‰
        self.u0 = nn.Parameter(
            torch.zeros(control_dim, dtype=torch.float32, device=self.device),
            requires_grad=True
        )

    def _build_branch(self, input_dim: int, hidden_dims: List[int], output_dim: int) -> nn.Sequential:
        """æ„å»ºå¤šå°ºåº¦åˆ†æ”¯ç½‘ç»œï¼ˆæ”¹è¿›2ï¼šæ··åˆæ¿€æ´»å‡½æ•°ï¼‰"""
        layers = []
        in_dim = input_dim
        
        for i, hidden_dim in enumerate(hidden_dims):
            layers.append(nn.Linear(in_dim, hidden_dim, device=self.device))
            # æ”¹è¿›2ï¼šå‰ä¸¤å±‚ç”¨tanhï¼ˆå±€éƒ¨éçº¿æ€§ï¼‰ï¼Œåä¸¤å±‚ç”¨Swishï¼ˆç¼“è§£æ¢¯åº¦é¥±å’Œï¼‰
            if i < 2:
                layers.append(nn.Tanh())
            else:
                layers.append(nn.SiLU())  # Swishæ¿€æ´»å‡½æ•°
            in_dim = hidden_dim
        
        # åˆ†æ”¯è¾“å‡ºå±‚
        layers.append(nn.Linear(in_dim, output_dim, device=self.device))
        return nn.Sequential(*layers)

    def normalize_x(self, x: torch.Tensor) -> torch.Tensor:
        """çŠ¶æ€å½’ä¸€åŒ–ï¼ˆä¿æŒåŸé€»è¾‘ï¼‰"""
        x = x.to(self.device, dtype=torch.float32)
        x_clamped = torch.clamp(x, self.low, self.high)
        x_norm = (x_clamped - self.low) / (self.high - self.low + 1e-8)
        return x_norm

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­ï¼ˆèåˆæ”¹è¿›1/2/3ï¼‰ï¼š
        1. çŠ¶æ€æ³¨æ„åŠ›åŠ æƒ
        2. å¤šå°ºåº¦åˆ†æ”¯ç‰¹å¾æå–
        3. ç‰¹å¾èåˆè¾“å‡º
        """
        # çŠ¶æ€å½’ä¸€åŒ–
        x_norm = self.normalize_x(x)
        # x_norm = x
        
        # æ”¹è¿›1ï¼šçŠ¶æ€æ³¨æ„åŠ›æœºåˆ¶ï¼ˆæƒé‡é€å…ƒç´ ç›¸ä¹˜ï¼‰
        attention_weights = self.attention(x_norm)  # [batch_size, input_dim]
        x_attended = x_norm * attention_weights     # å…³é”®ç»´åº¦å¢å¼ºï¼Œæ¬¡è¦ç»´åº¦æŠ‘åˆ¶
        
        # æ”¹è¿›3ï¼šå¤šå°ºåº¦ç‰¹å¾èåˆ
        feat_low = self.branch_low(x_attended)   # æ…¢å˜ç‰¹å¾ï¼ˆå¦‚ä½ç½®yã€è§’åº¦Î¸ï¼‰
        feat_high = self.branch_high(x_attended) # å¿«å˜ç‰¹å¾ï¼ˆå¦‚è§’é€Ÿåº¦Î¸_dotã€é€Ÿåº¦áºï¼‰
        psi_x = torch.cat([feat_low, feat_high], dim=1)  # èåˆç‰¹å¾
        
        return psi_x

    def compute_z(self, x: torch.Tensor, x_star: torch.Tensor) -> torch.Tensor:
        """è®¡ç®—é«˜ç»´çº¿æ€§ç©ºé—´çŠ¶æ€zï¼ˆä¿æŒåŸé€»è¾‘ï¼‰"""
        x_star = x_star.to(self.device, dtype=torch.float32)
        if x_star.dim() == 1:
            x_star = x_star.unsqueeze(0)
        x_star_batch = x_star.expand(x.shape[0], -1)
        
        psi_x = self.forward(x)
        psi_x_star = self.forward(x_star_batch)
        return psi_x - psi_x_star

    def forward_u0(self, x: torch.Tensor) -> torch.Tensor:
        """è¾“å‡ºæ§åˆ¶è¾“å…¥å›ºå®šç‚¹uâ‚€ï¼ˆä¿æŒåŸé€»è¾‘ï¼‰"""
        batch_size = x.shape[0]
        return self.u0.unsqueeze(0).expand(batch_size, -1)


class PsiMLP_v3(nn.Module):
    """
    æœ€ç»ˆç‰ˆKoopmanåŸºå‡½æ•°å­¦ä¹ ç½‘ç»œï¼ˆå®Œå…¨é€‚é…æ–‡æ¡£åœºæ™¯ï¼‰
    æ ¸å¿ƒæ”¹è¿›ä¾æ®æ–‡æ¡£ï¼š
    1. ç‰©ç†å¯¹ç§°ç‰¹å¾æå–ï¼ˆIV.DèŠ‚Lunar LanderçŠ¶æ€å®šä¹‰ï¼šx,y,Î¸,áº‹,áº,Î¸Ì‡ï¼Œè§£å†³Î¸ä¸è¿ç»­é—®é¢˜ï¼‰
    2. æµå½¢ç“¶é¢ˆå±‚ï¼ˆII.20èŠ‚Koopmançº¿æ€§æå‡éœ€ä¿ç•™ç³»ç»Ÿå†…åœ¨åŠ¨æ€ï¼Œçº¦æŸä½ç»´æµå½¢ç»´åº¦d=4ï¼‰
    3. æ§åˆ¶æ•æ„Ÿæ€§æ³¨æ„åŠ›ï¼ˆIIIèŠ‚Koopman-basedæ§åˆ¶ï¼Œé€‚é…IV.DèŠ‚u1å½±å“yã€u2å½±å“xçš„æ§åˆ¶å…³è”ï¼‰
    4. èƒ½æ§æ€§æ­£åˆ™ï¼ˆIIIèŠ‚LQR/MPCä¾èµ–èƒ½æ§æ€§ï¼Œçº¦æŸABçŸ©é˜µèƒ½æ§æ€§çŸ©é˜µç§©ï¼‰
    5. è”åˆé‡æ„å¤´ï¼ˆEquation 9ï¼šCçŸ©é˜µé‡æ„åŸçŠ¶æ€ï¼Œçº¦æŸzç©ºé—´ä¸åŸçŠ¶æ€æµå½¢ä¸€è‡´ï¼‰
    6. è´å¶æ–¯æ³¨æ„åŠ›ï¼ˆé™ä½Algorithm 1è®­ç»ƒçš„ç§å­æ•æ„Ÿæ€§ï¼Œç¡®ä¿å¤šå›åˆç¨³å®šæ€§ï¼‰
    """
    def __init__(
        self,
        input_dim: int,
        output_dim: int,
        control_dim: int,
        physics_dim: int,
        low: Union[List[float], np.ndarray],
        high: Union[List[float], np.ndarray],
        hidden_dims: List[int] = None,
        device: Optional[torch.device] = None
    ):
        super().__init__()
        self.device = device or torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # æ ¸å¿ƒå‚æ•°
        self.input_dim = input_dim  # åŸçŠ¶æ€ç»´åº¦ï¼ˆLunar Landerï¼š6ç»´ï¼ŒğŸ”¶1-80ï¼‰
        self.output_dim = output_dim  # é«˜ç»´æå‡ç©ºé—´ç»´åº¦ï¼ˆNâ‰«6ï¼Œå¦‚256ï¼‰
        self.control_dim = control_dim  # æ§åˆ¶ç»´åº¦ï¼ˆLunar Landerï¼š2ç»´ï¼ŒğŸ”¶1-80ï¼‰
        
        # çŠ¶æ€å½’ä¸€åŒ–å‚æ•°ï¼ˆè§£è€¦IV.DèŠ‚OpenAI Gymç¯å¢ƒä¾èµ–ï¼Œçº¦æŸçŠ¶æ€èŒƒå›´ï¼‰
        self.low = torch.tensor(low, dtype=torch.float32, device=self.device)
        self.high = torch.tensor(high, dtype=torch.float32, device=self.device)
        assert self.low.shape[0] == input_dim and self.high.shape[0] == input_dim, \
            f"low/highç»´åº¦éœ€ä¸input_dim({input_dim})ä¸€è‡´ï¼ˆæ–‡æ¡£IV.DèŠ‚çŠ¶æ€ç»´åº¦ä¸º6ï¼‰"
        
        # æ”¹è¿›1ï¼šè´å¶æ–¯çŠ¶æ€æ³¨æ„åŠ›ï¼ˆé™ä½ç§å­æ•æ„Ÿæ€§ï¼Œé‡åŒ–æƒé‡ä¸ç¡®å®šæ€§ï¼‰
        self.attention_mu = nn.Linear(input_dim, input_dim, device=self.device)  # æ³¨æ„åŠ›å‡å€¼
        self.attention_logvar = nn.Linear(input_dim, input_dim, device=self.device)  # æ³¨æ„åŠ›å¯¹æ•°æ–¹å·®
        # åŠ å…¥æ§åˆ¶æ•æ„Ÿæ³¨æ„åŠ›æœºåˆ¶
        self.control_sensitive_prior = nn.Parameter(
            torch.tensor([0.4, 0.4, 0.1, 0.1, 0.4, 0.1], device=self.device),  # é¢„å­¦ä¹ æ§åˆ¶æ•æ„Ÿå…ˆéªŒï¼ˆx/yæƒé‡é«˜ï¼ŒğŸ”¶1-80ï¼‰
            requires_grad=False
        )
        
        # æ”¹è¿›2ï¼šå¤šå°ºåº¦åˆ†æ”¯ç»“æ„
        self.hidden_dims = hidden_dims or [256, 256, 256, 256]  # æ–‡æ¡£II.28èŠ‚æ¨è4å±‚éšè—å±‚
        # ä½ç»´åˆ†æ”¯ï¼ˆæ•æ‰æ…¢å˜ç‰¹å¾ï¼šx,y,Î¸ï¼Œåˆ†é…1/4ç»´åº¦ï¼‰
        self.branch_low = self._build_branch(
            input_dim=input_dim,
            hidden_dims=self.hidden_dims,
            output_dim=output_dim // 4,
            physics_dim=4
        )
        # é«˜ç»´åˆ†æ”¯ï¼ˆæ•æ‰å¿«å˜ç‰¹å¾ï¼šáº‹,áº,Î¸Ì‡ï¼Œåˆ†é…3/4ç»´åº¦ï¼‰
        self.branch_high = self._build_branch(
            input_dim=input_dim,
            hidden_dims=self.hidden_dims,
            output_dim=output_dim - (output_dim // 4),
            physics_dim=4
        )
        
        # æ”¹è¿›3ï¼šè”åˆé‡æ„å¤´ï¼ˆå¯¹åº”æ–‡æ¡£Equation 9çš„CçŸ©é˜µï¼Œè”åˆè®­ç»ƒçº¦æŸz-åŸçŠ¶æ€æµå½¢ä¸€è‡´ï¼‰
        self.recon_head = nn.Sequential(
            nn.Linear(output_dim, self.hidden_dims[-1], device=self.device),
            nn.SiLU(),  # å¤ç”¨æ··åˆæ¿€æ´»ä¼˜åŠ¿ï¼Œç¼“è§£æ¢¯åº¦é¥±å’Œ
            nn.Linear(self.hidden_dims[-1], input_dim, device=self.device)  # è¾“å‡ºåŸçŠ¶æ€ç»´åº¦ï¼ˆ6ï¼‰
        )
        
        # æ§åˆ¶å›ºå®šç‚¹uâ‚€ï¼ˆæ–‡æ¡£II.36èŠ‚ï¼šè¾…åŠ©ç½‘ç»œå­¦ä¹ éé›¶å›ºå®šç‚¹ï¼Œé€‚é…æ§åˆ¶ affine ç³»ç»Ÿï¼‰
        self.u0 = nn.Parameter(
            torch.zeros(control_dim, dtype=torch.float32, device=self.device),
            requires_grad=True
        )

    def _build_branch(self, input_dim: int, hidden_dims: List[int], output_dim: int, physics_dim: int) -> nn.Sequential:
        """
        æ„å»ºå¤šå°ºåº¦åˆ†æ”¯ï¼ˆæ”¹è¿›ï¼šåŠ å…¥æµå½¢ç“¶é¢ˆå±‚ï¼Œçº¦æŸç³»ç»Ÿå†…åœ¨ç»´åº¦d=4ï¼ŒğŸ”¶1-20èŠ‚Koopmançº¿æ€§æå‡æ ¸å¿ƒï¼‰
        d=4ï¼šåŸºäºIV.DèŠ‚Lunar LanderçŠ¶æ€åˆ†æï¼Œå†…åœ¨åŠ¨æ€ç»´åº¦çº¦3~4ï¼ˆä½ç½®-é€Ÿåº¦è€¦åˆï¼‰
        """
        layers = []
        in_dim = input_dim
        
        # åŸéšè—å±‚ï¼ˆä¿ç•™v2æ··åˆæ¿€æ´»ï¼šå‰2å±‚tanhå±€éƒ¨éçº¿æ€§ï¼Œå2å±‚SiLUç¼“è§£é¥±å’Œï¼ŒğŸ”¶1-28èŠ‚è®­ç»ƒç¨³å®šæ€§ï¼‰
        for i, hidden_dim in enumerate(hidden_dims):
            layers.append(nn.Linear(in_dim, hidden_dim, device=self.device))
            layers.append(nn.Tanh() if i < 2 else nn.SiLU())
            in_dim = hidden_dim
        
        # æ ¸å¿ƒæ”¹è¿›ï¼šæµå½¢ç“¶é¢ˆå±‚ï¼ˆå¼ºåˆ¶å‹ç¼©åˆ°å†…åœ¨ç»´åº¦ï¼Œå‡å°‘å†—ä½™ç»´åº¦ï¼‰
        layers.append(nn.Linear(in_dim, physics_dim, device=self.device))
        layers.append(nn.SiLU())  # ä¿ç•™éçº¿æ€§è¡¨è¾¾
        
        # æ¢å¤åˆ°åˆ†æ”¯è¾“å‡ºç»´åº¦ï¼ˆä»ä½ç»´æµå½¢æ‰©å±•åˆ°é«˜ç»´zç©ºé—´ï¼Œç¡®ä¿çº¿æ€§æå‡æœ‰æ•ˆæ€§ï¼ŒğŸ”¶1-20èŠ‚ï¼‰
        layers.append(nn.Linear(physics_dim, output_dim, device=self.device))
        return nn.Sequential(*layers)

    def normalize_x(self, x: torch.Tensor) -> torch.Tensor:
        """
        çŠ¶æ€å½’ä¸€åŒ–+ç‰©ç†å¯¹ç§°ç‰¹å¾æå–ï¼ˆæ”¹è¿›ï¼šé€‚é…IV.DèŠ‚Lunar LanderçŠ¶æ€ç‰¹æ€§ï¼‰
        è§£å†³Î¸åœ¨Â±Ï€å¤„ä¸è¿ç»­é—®é¢˜ï¼ˆæ›¿æ¢Î¸ä¸ºcosÎ¸ï¼Œç¡®ä¿æ—‹è½¬å¯¹ç§°æ€§ï¼ŒğŸ”¶1-80èŠ‚çŠ¶æ€å®šä¹‰ï¼‰
        æ ¹æ®å®é™…æƒ…å†µï¼Œéœ€è¦é‡æ–°å†™normalize_x
        """
        x = x.to(self.device, dtype=torch.float32)
        x_clamped = torch.clamp(x, self.low, self.high)  # çº¦æŸåœ¨ç¯å¢ƒåˆæ³•èŒƒå›´
        
        # ç‰©ç†å¯¹ç§°ç‰¹å¾é‡ç»„ï¼ˆä¿ç•™6ç»´ï¼Œä¸åŸè¾“å…¥å…¼å®¹ï¼‰
        x_rel = x_clamped[:, 0:1] - 0.0  # xç›¸å¯¹ç›®æ ‡åç§»ï¼ˆIV.DèŠ‚ç›®æ ‡x=0ï¼ŒğŸ”¶1-80ï¼‰
        y_rel = x_clamped[:, 1:2] - 0.0  # yç›¸å¯¹ç›®æ ‡åç§»ï¼ˆIV.DèŠ‚ç›®æ ‡y=0ï¼ŒğŸ”¶1-80ï¼‰
        dot_x = x_clamped[:, 2:3]  # ä¿ç•™å¿«å˜ç‰¹å¾áº‹
        dot_y = x_clamped[:, 3:4]  # ä¿ç•™å¿«å˜ç‰¹å¾áº
        cos_theta = torch.cos(x_clamped[:, 4:5])  # æ›¿æ¢åŸå§‹Î¸ï¼Œé¿å…æ—‹è½¬ä¸è¿ç»­
        dot_theta = x_clamped[:, 5:6]  # ä¿ç•™å¿«å˜ç‰¹å¾Î¸Ì‡
        
        # é‡ç»„ä¸º6ç»´å¯¹ç§°åŒ–çŠ¶æ€ï¼ˆå¯¹é½IV.DèŠ‚è¾“å…¥ç»´åº¦ï¼ŒğŸ”¶1-80ï¼‰
        x_symmetric = torch.cat([x_rel, y_rel, dot_x, dot_y, cos_theta, dot_theta], dim=1)
        # å½’ä¸€åŒ–åˆ°[0,1]ï¼ˆå‡å°‘ç¯å¢ƒå°ºåº¦å½±å“ï¼ŒğŸ”¶1-28èŠ‚è®­ç»ƒç¨³å®šæ€§ï¼‰
        x_norm = (x_symmetric - self.low) / (self.high - self.low + 1e-8)
        return x_norm

    def _compute_attention(self, x_norm: torch.Tensor, u: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        è®¡ç®—è´å¶æ–¯+æ§åˆ¶æ•æ„Ÿæ€§æ³¨æ„åŠ›ï¼ˆæ”¹è¿›ï¼šå¯¹é½IIIèŠ‚Koopmanæ§åˆ¶éœ€æ±‚ï¼ŒğŸ”¶1-43èŠ‚ï¼‰
        è¿”å›ï¼šæ³¨æ„åŠ›æƒé‡ã€æ³¨æ„åŠ›ä¸ç¡®å®šæ€§æŸå¤±
        """
        batch_size = x_norm.shape[0]
        
        # 1. è´å¶æ–¯æ³¨æ„åŠ›é‡‡æ ·ï¼ˆé‡å‚æ•°åŒ–æŠ€å·§ï¼Œé™ä½ç§å­æ•æ„Ÿæ€§ï¼‰
        attn_mu = self.attention_mu(x_norm)
        attn_logvar = self.attention_logvar(x_norm)
        attn_std = torch.exp(0.5 * attn_logvar)
        eps = torch.randn_like(attn_std)
        base_attention = torch.sigmoid(attn_mu + eps * attn_std)  # æƒé‡âˆˆ[0,1]
        
        # 2. æ³¨æ„åŠ›ä¸ç¡®å®šæ€§æŸå¤±ï¼ˆæ­£åˆ™åŒ–å‡å€¼å’Œæ–¹å·®ï¼Œç¡®ä¿æƒé‡ç¨³å®šï¼‰
        attn_uncert_loss = torch.mean(attn_logvar + (attn_mu ** 2))  # é¿å…æç«¯æƒé‡
        
        # 3. æ§åˆ¶æ•æ„Ÿæ€§æ³¨æ„åŠ›ï¼ˆè®­ç»ƒæ—¶ç”Ÿæ•ˆï¼Œé€‚é…IV.DèŠ‚uä¸çŠ¶æ€çš„å…³è”ï¼‰
        if u is not None and self.training:
            # u1ï¼ˆä¸»å¼•æ“ï¼‰å½±å“yï¼ˆy_relï¼šx_norm[:,1]ï¼‰ï¼Œu2ï¼ˆä¾§å¼•æ“ï¼‰å½±å“xï¼ˆx_relï¼šx_norm[:,0]ï¼‰ï¼ŒğŸ”¶1-80
            u1 = u[:, 0:1]
            u2 = u[:, 1:2]
            # æ§åˆ¶æ•æ„Ÿæƒé‡ï¼šuè¶Šå¤§ï¼Œå¯¹åº”çŠ¶æ€ç»´åº¦æƒé‡è¶Šé«˜
            control_sensitive = torch.zeros_like(base_attention)
            control_sensitive[:, 0:1] = u2  # xç»´åº¦å…³è”u2
            control_sensitive[:, 1:2] = u1  # yç»´åº¦å…³è”u1
            control_sensitive[:, 4:5] = (u1 + u2) / 2  # Î¸Ì‡ç»´åº¦å…³è”æ€»æ¨åŠ›
            control_sensitive = torch.sigmoid(control_sensitive * 5)  # æ”¾å¤§å·®å¼‚
            
            # èåˆæ³¨æ„åŠ›ï¼ˆæ§åˆ¶æ•æ„Ÿç»´åº¦æƒé‡å¢å¼ºï¼‰
            attention_weights = base_attention * (1 + control_sensitive)
            attention_weights = attention_weights / attention_weights.sum(dim=1, keepdim=True)  # å½’ä¸€åŒ–
        else:
            # æµ‹è¯•æ—¶ï¼šç”¨é¢„å­¦ä¹ æ§åˆ¶æ•æ„Ÿå…ˆéªŒï¼ˆæ— uè¾“å…¥æ—¶ä¿æŒæ§åˆ¶å…³è”æ€§ï¼ŒğŸ”¶1-83èŠ‚æµ‹è¯•é€»è¾‘ï¼‰
            attention_weights = base_attention * self.control_sensitive_prior.unsqueeze(0)
        
        return attention_weights, attn_uncert_loss

    def _compute_controllability_loss(self, psi_x: torch.Tensor, u: torch.Tensor) -> torch.Tensor:
        """
        èƒ½æ§æ€§æ­£åˆ™æŸå¤±ï¼ˆæ”¹è¿›ï¼šç¡®ä¿zç©ºé—´åŒ…å«æ§åˆ¶æ•æ„Ÿæ–¹å‘ï¼Œé€‚é…IIIèŠ‚LQR/MPCï¼ŒğŸ”¶1-44èŠ‚ï¼‰
        çº¦æŸï¼šæ§åˆ¶è¾“å…¥uå˜åŒ–æ—¶ï¼Œzï¼ˆpsi_xï¼‰éœ€åŒæ­¥å˜åŒ–ï¼Œé¿å…BçŸ©é˜µå¤±æ•ˆ
        """
        # è®¡ç®—uå’Œzçš„æ–¹å·®ï¼ˆåæ˜ å˜åŒ–èŒƒå›´ï¼‰
        u_var = u.var(dim=0, keepdim=True)  # æ§åˆ¶è¾“å…¥æ–¹å·®
        z_var = psi_x.var(dim=0, keepdim=True)  # zç©ºé—´æ–¹å·®
        
        # ä»…å½“uæœ‰å˜åŒ–æ—¶ï¼ˆu_var>0.01ï¼‰ï¼Œçº¦æŸzçš„å˜åŒ–é‡ï¼ˆé¿å…uæ’å®šæ—¶è¯¯åˆ¤ï¼‰
        mask = (u_var > 0.01).float().unsqueeze(0)
        # æƒ©ç½šzå˜åŒ–è¿‡å°ï¼ˆç›®æ ‡z_varâ‰¥0.1ï¼Œé€‚é…IV.DèŠ‚æ§åˆ¶èŒƒå›´u1âˆˆ[0,1],u2âˆˆ[-1,1]ï¼‰
        controllability_loss = mask * torch.maximum(torch.zeros_like(z_var), 0.1 - z_var).mean()
        return controllability_loss

    def forward(self, x: torch.Tensor, u: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­ï¼ˆæ ¸å¿ƒæµç¨‹ï¼šå¯¹ç§°åŒ–â†’æ³¨æ„åŠ›â†’å¤šå°ºåº¦ç‰¹å¾â†’æ­£åˆ™æŸå¤±ï¼Œå¯¹é½Algorithm 1æ­¥éª¤1-4ï¼‰
        è¿”å›ï¼šKoopmanåŸºå‡½æ•°è¾“å‡ºpsi_xã€æ€»æ­£åˆ™æŸå¤±ï¼ˆä¸ç¡®å®šæ€§+èƒ½æ§æ€§ï¼‰
        """
        # 1. çŠ¶æ€å¯¹ç§°åŒ–ä¸å½’ä¸€åŒ–
        x_norm = self.normalize_x(x)
        # 2. æ³¨æ„åŠ›è®¡ç®—ï¼ˆè´å¶æ–¯+æ§åˆ¶æ•æ„Ÿæ€§ï¼‰
        attention_weights, attn_uncert_loss = self._compute_attention(x_norm, u)
        x_attended = x_norm * attention_weights  # å¢å¼ºå…³é”®ç»´åº¦
        # 3. å¤šå°ºåº¦ç‰¹å¾èåˆï¼ˆæ…¢å˜+å¿«å˜ï¼ŒğŸ”¶1-21èŠ‚åŠ¨æ€æ•æ‰ï¼‰
        feat_low = self.branch_low(x_attended)   # æ…¢å˜ï¼šx,y,Î¸
        feat_high = self.branch_high(x_attended) # å¿«å˜ï¼šáº‹,áº,Î¸Ì‡
        psi_x = torch.cat([feat_low, feat_high], dim=1)
        # 4. èƒ½æ§æ€§æ­£åˆ™æŸå¤±ï¼ˆä»…è®­ç»ƒæ—¶è®¡ç®—ï¼ŒğŸ”¶1-43èŠ‚èƒ½æ§æ€§è¦æ±‚ï¼‰
        controllability_loss = torch.tensor(0.0, device=self.device)
        if u is not None and self.training:
            controllability_loss = self._compute_controllability_loss(psi_x, u)
        # æ€»æ­£åˆ™æŸå¤±ï¼ˆèåˆä¸ç¡®å®šæ€§ä¸èƒ½æ§æ€§çº¦æŸï¼‰
        total_reg_loss = 0.01 * attn_uncert_loss + 0.5 * controllability_loss
        total_reg_loss = total_reg_loss.squeeze(0)
        total_reg_loss = torch.sum(total_reg_loss, dim=1) if len(total_reg_loss.shape) > 1 else total_reg_loss

        return psi_x, total_reg_loss

    def forward_with_recon(self, x: torch.Tensor, u: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        è”åˆè®­ç»ƒæ¥å£ï¼ˆæ”¹è¿›ï¼šå¯¹é½Equation 9ï¼Œè”åˆä¼˜åŒ–KoopmanåŸºå‡½æ•°ä¸çŠ¶æ€é‡æ„ï¼ŒğŸ”¶1-42èŠ‚ï¼‰
        è¿”å›ï¼špsi_xã€æ€»æ­£åˆ™æŸå¤±ã€é‡æ„æŸå¤±
        """
        # 1. åŸºç¡€å‰å‘ä¼ æ’­ï¼ˆè·å–psi_xä¸æ­£åˆ™æŸå¤±ï¼‰
        psi_x, total_reg_loss = self.forward(x, u)
        # 2. çŠ¶æ€é‡æ„ï¼ˆå¯¹åº”CçŸ©é˜µï¼šx â‰ˆ CÂ·psi_xï¼ŒğŸ”¶1-42èŠ‚ï¼‰
        x_recon = self.recon_head(psi_x)
        x_norm = self.normalize_x(x)
        # 3. é‡æ„æŸå¤±ï¼ˆMSEï¼Œçº¦æŸzç©ºé—´ä¸åŸçŠ¶æ€æµå½¢ä¸€è‡´ï¼‰
        recon_weights = torch.tensor([3.0, 3.0, 1.0, 1.0, 3.0, 1.0], device=self.device).unsqueeze(0)
        # åŠ æƒMSEæŸå¤±ï¼ˆä»…æƒ©ç½šæ ¸å¿ƒç»´åº¦çš„é‡æ„è¯¯å·®ï¼‰
        recon_loss = torch.mean(torch.square(x_recon - x_norm) * recon_weights)
            
        return psi_x, total_reg_loss.item(), recon_loss.item()

    def compute_z(self, x: torch.Tensor, x_star: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—é«˜ç»´çº¿æ€§çŠ¶æ€zï¼ˆæ–‡æ¡£Equation 4ï¼šz=Î¨(x)-Î¨(x*)ï¼ŒğŸ”¶1-35èŠ‚æ ¸å¿ƒçº¿æ€§åŒ–æ­¥éª¤ï¼‰
        x_starï¼šç›®æ ‡çŠ¶æ€ï¼ˆIV.DèŠ‚Lunar Landerç€é™†åŒºï¼šx*=[0,0,0,0,0,0]ï¼‰
        """
        x_star = x_star.to(self.device, dtype=torch.float32)
        # æ‰©å±•x_staråˆ°æ‰¹æ¬¡ç»´åº¦
        if x_star.dim() == 1:
            x_star = x_star.unsqueeze(0)
        x_star_batch = x_star.expand(x.shape[0], -1)
        
        # è®¡ç®—Î¨(x)ä¸Î¨(x*)
        psi_x, _ = self.forward(x)  # å¿½ç•¥æ­£åˆ™æŸå¤±ï¼ˆä»…è®¡ç®—zæ—¶æ— éœ€ï¼‰
        psi_x_star, _ = self.forward(x_star_batch)
        return psi_x - psi_x_star

    def forward_u0(self, x: torch.Tensor) -> torch.Tensor:
        """
        è¾“å‡ºæ§åˆ¶å›ºå®šç‚¹uâ‚€ï¼ˆæ–‡æ¡£II.36èŠ‚ï¼šè¾…åŠ©ç½‘ç»œå­¦ä¹ uâ‚€ï¼Œé€‚é… affine å˜æ¢v=u-uâ‚€ï¼ŒğŸ”¶1-35èŠ‚ï¼‰
        æ‰©å±•åˆ°æ‰¹æ¬¡ç»´åº¦ï¼ŒåŒ¹é…è¾“å…¥æ‰¹é‡å¤§å°
        """
        batch_size = x.shape[0]
        return self.u0.unsqueeze(0).expand(batch_size, -1)
    
